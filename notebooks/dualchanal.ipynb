{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyON9FUramayERyNTkD2KmWM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/younesabdolmalaky/A-Dual-Channel-Approach-for-Farsi-Text-Classification-using-Transfer-Learning-Techniques/blob/main/notebooks/dualchanal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLJcpcvVjaMT",
        "outputId": "79991cea-94b2-47be-8bd1-cf204d386569"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input , Dense ,Dropout\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from scipy.sparse import vstack\n",
        "import bz2\n",
        "import numpy as np\n",
        "import pickle\n",
        "import mmap \n",
        "import re"
      ],
      "metadata": {
        "id": "GwN3-46jjlsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_FEATURES = 12000\n",
        "sequences = Input(shape=(255,))\n",
        "embedded = layers.Embedding(MAX_FEATURES, 32)(sequences)\n",
        "x = layers.Conv1D(64, 3, activation='relu')(embedded)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.MaxPool1D(3)(x)\n",
        "x = layers.Conv1D(32, 5, activation='relu')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.MaxPool1D(5)(x)\n",
        "x = layers.Conv1D(16, 5, activation='relu')(x)\n",
        "x = layers.GlobalMaxPool1D()(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(64, activation='relu')(x)"
      ],
      "metadata": {
        "id": "RTQ8ZD-EjnCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = Input(shape=(905917,))\n",
        "x1 = layers.Dense(128, activation='relu')(tfidf)\n",
        "x1 = layers.Dense(64, activation='relu')(x1)"
      ],
      "metadata": {
        "id": "TEaFkJm0joRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged = layers.concatenate([x, x1])"
      ],
      "metadata": {
        "id": "a4RCeqlCjpZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense1 = Dense(256, activation='relu')(merged)\n",
        "dense1 = Dropout(0.1)(dense1)\n",
        "dense1 = Dense(32, activation='relu')(dense1)\n",
        "outputs = Dense(1, activation='sigmoid')(dense1)"
      ],
      "metadata": {
        "id": "k1R3xrvUjqhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs=[sequences, tfidf], outputs=outputs)"
      ],
      "metadata": {
        "id": "_osRe3aejsYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mch = callbacks.ModelCheckpoint('../models/fianl-model.h5' , monitor='accuracy' , mode ='max' , save_best_only=True)"
      ],
      "metadata": {
        "id": "RdXtnMAojyuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "eV2LO2NajyzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DualChannelDataGenerator(Sequence):\n",
        "    def __init__(self, dataset_size , seq_x_file, tfidf_x_file, y_file, batch_size):\n",
        "      self.dataset_size = dataset_size\n",
        "      with open(seq_x_file, 'rb') as f:\n",
        "          mm = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)\n",
        "          self.seq_x = np.load(mm, allow_pickle=True)\n",
        "      with open(tfidf_x_file, 'rb') as f:\n",
        "          mm = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)\n",
        "          self.tfidf_x = np.load(mm, allow_pickle=True)\n",
        "      with open(y_file, 'rb') as f:\n",
        "          mm = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)\n",
        "          self.y = np.load(mm, allow_pickle=True)\n",
        "      self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(self.dataset_size/ float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_seq_x = self.seq_x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_tfidf_x = self.tfidf_x[idx * self.batch_size:(idx + 1) * self.batch_size].toarray()\n",
        "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        return [batch_seq_x, batch_tfidf_x], batch_y\n",
        "\n"
      ],
      "metadata": {
        "id": "YIn0ZQC3jy31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/persian-sentiment-analysis/X_train_tfidf1.pickle' , 'rb') as f:\n",
        "  tfidf_1 = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/persian-sentiment-analysis/X_train_tfidf2.pickle' , 'rb') as f:\n",
        "  tfidf_2 = pickle.load(f)\n",
        "\n",
        "with open('/content/X_train_tfidf.pickle' , 'wb') as f:\n",
        "  pickle.dump(vstack([tfidf_1 , tfidf_2]) , f)\n",
        "\n",
        "del tfidf_1\n",
        "del tfidf_2"
      ],
      "metadata": {
        "id": "0cBKnqVljy8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/persian-sentiment-analysis/X_test_tfidf.pickle' , 'rb') as f:\n",
        "  tfidf = pickle.load(f)\n",
        "\n",
        "with open('/content/X_test_tfidf.pickle' , 'wb') as f:\n",
        "  pickle.dump(tfidf , f)\n",
        "\n",
        "del tfidf\n"
      ],
      "metadata": {
        "id": "8YrJcW3YjzAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/persian-sentiment-analysis/train_pad_sequences1.pickle' , 'rb') as f:\n",
        "  seq_1 = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/persian-sentiment-analysis/train_pad_sequences2.pickle' , 'rb') as f:\n",
        "  seq_2 = pickle.load(f)\n",
        "\n",
        "with open('/content/train_pad_sequences.pickle' , 'wb') as f:\n",
        "  pickle.dump(np.concatenate((seq_1 , seq_2), axis=0) , f)\n",
        "\n",
        "del seq_1\n",
        "del seq_2"
      ],
      "metadata": {
        "id": "bf8NCLshjzF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/persian-sentiment-analysis/test_pad_sequences.pickle' , 'rb') as f:\n",
        "  seq_test = pickle.load(f)\n",
        "\n",
        "with open('/content/test_pad_sequences.pickle' , 'wb') as f:\n",
        "  pickle.dump(seq_test , f)\n",
        "\n",
        "del seq_test"
      ],
      "metadata": {
        "id": "BXDKFjg-jzLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/persian-sentiment-analysis/train_labels.pickle' , 'rb') as f:\n",
        "  train_labels = pickle.load(f)\n",
        "\n",
        "with open('/content/train_labels.pickle' , 'wb') as f:\n",
        "  pickle.dump(train_labels, f)\n",
        "\n",
        "del train_labels"
      ],
      "metadata": {
        "id": "ivk9U33tj5Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/persian-sentiment-analysis/test_labels.pickle' , 'rb') as f:\n",
        "  test_labels = pickle.load(f)\n",
        "\n",
        "with open('/content/test_labels.pickle' , 'wb') as f:\n",
        "  pickle.dump(test_labels , f)\n",
        "\n",
        "del test_labels"
      ],
      "metadata": {
        "id": "hhqrjH5Uj5Q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "dataset_size_train = 3600000\n",
        "dataset_size_test = 400000\n",
        "seq_path_train = '/content/train_pad_sequences.pickle'\n",
        "seq_path_test =  '/content/test_pad_sequences.pickle'\n",
        "tfidf_train = '/content/X_train_tfidf.pickle'\n",
        "tfidf_test = '/content/X_test_tfidf.pickle'\n",
        "train_labels = '/content/train_labels.pickle'\n",
        "test_labels = '/content/test_labels.pickle'"
      ],
      "metadata": {
        "id": "1ZthQiX_j5W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = DualChannelDataGenerator(dataset_size_train ,seq_path_train , tfidf_train , train_labels , batch_size)"
      ],
      "metadata": {
        "id": "6FWrhYBOj8pC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator = DualChannelDataGenerator(dataset_size_test ,seq_path_test , tfidf_test , test_labels , batch_size)"
      ],
      "metadata": {
        "id": "S-rq9Hgaj8tK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_generator, steps_per_epoch=len(train_generator), epochs=100 , validation_data=test_generator, validation_steps=len(test_generator),callbacks=[mch])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oetG0n_Jj8yN",
        "outputId": "5a2d8ab9-3102-47af-b19b-fca1a061d542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            " 8586/28125 [========>.....................] - ETA: 3:47:31 - loss: 0.1859 - accuracy: 0.9267"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "afntIpfSj84I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pdsq1__2j5cz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}