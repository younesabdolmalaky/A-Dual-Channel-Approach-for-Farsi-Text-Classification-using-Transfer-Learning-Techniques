{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1haVZM2iZLjLEM8g4ndIcLy95JhkjgPTI",
      "authorship_tag": "ABX9TyNCdBJLNsDLE4al+3788X4+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/younesabdolmalaky/A-Dual-Channel-Approach-for-Farsi-Text-Classification-using-Transfer-Learning-Techniques/blob/main/notebooks/data_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkpuvZTmiD9F",
        "outputId": "612ef984-d8e1-4d49-e2c7-c4f06fdca87f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "Downloading amazonreviews.zip to /content\n",
            " 99% 487M/493M [00:04<00:00, 155MB/s]\n",
            "100% 493M/493M [00:04<00:00, 116MB/s]\n",
            "Archive:  amazonreviews.zip\n",
            "  inflating: test.ft.txt.bz2         \n",
            "  inflating: train.ft.txt.bz2        \n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download bittlingmayer/amazonreviews\n",
        "! unzip amazonreviews.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import bz2\n",
        "import re\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "tycBnPAgiIY7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_labels_and_texts(file):\n",
        "    labels = []\n",
        "    texts = []\n",
        "    for line in bz2.BZ2File(file):\n",
        "        x = line.decode(\"utf-8\")\n",
        "        labels.append(int(x[9]) - 1)\n",
        "        texts.append(x[10:].strip())\n",
        "    return np.array(labels), texts\n",
        "train_labels, train_texts = get_labels_and_texts('train.ft.txt.bz2')\n",
        "test_labels, test_texts = get_labels_and_texts('test.ft.txt.bz2')\n"
      ],
      "metadata": {
        "id": "a3dy_oBkiSBA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NON_ALPHANUM = re.compile(r'[\\W]')\n",
        "NON_ASCII = re.compile(r'[^a-z0-1\\s]')\n",
        "def normalize_texts(texts):\n",
        "    normalized_texts = []\n",
        "    for text in texts:\n",
        "        lower = text.lower()\n",
        "        no_punctuation = NON_ALPHANUM.sub(r' ', lower)\n",
        "        no_non_ascii = NON_ASCII.sub(r'', no_punctuation)\n",
        "        normalized_texts.append(no_non_ascii)\n",
        "    return normalized_texts\n",
        "\n",
        "train_texts = normalize_texts(train_texts)\n",
        "test_texts = normalize_texts(test_texts)"
      ],
      "metadata": {
        "id": "8MXP-XdNiTPe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 10000)\n",
        "X_train = vectorizer.fit_transform(train_texts)\n",
        "X_test = vectorizer.transform(test_texts)"
      ],
      "metadata": {
        "id": "bwBOVQ7oiUYm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/persian-sentiment-analysis/X_train_tfidf.pickle', 'wb') as handle:\n",
        "    pickle.dump(X_train ,handle)\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/persian-sentiment-analysis/X_test_tfidf.pickle', 'wb') as handle:\n",
        "    pickle.dump(X_test, handle)\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/persian-sentiment-analysis/vectorizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(vectorizer, handle)"
      ],
      "metadata": {
        "id": "L6TR6Xq7j861"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del X_train\n",
        "del X_test\n",
        "del vectorizer"
      ],
      "metadata": {
        "id": "nnqfEazbiUxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_FEATURES = 12000\n",
        "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
        "tokenizer.fit_on_texts(train_texts)"
      ],
      "metadata": {
        "id": "tzGHl4h-icld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts = tokenizer.texts_to_sequences(train_texts)\n",
        "test_texts = tokenizer.texts_to_sequences(test_texts)"
      ],
      "metadata": {
        "id": "AWi7DqCCidAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = max(len(train_ex) for train_ex in train_texts)\n",
        "train_texts = pad_sequences(train_texts, maxlen=MAX_LENGTH)\n",
        "test_texts = pad_sequences(test_texts, maxlen=MAX_LENGTH)"
      ],
      "metadata": {
        "id": "TJUWYqezidGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH"
      ],
      "metadata": {
        "id": "ahVWHos7idMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/persian-sentiment-analysis/train_pad_sequences.pickle', 'wb') as handle:\n",
        "    pickle.dump(train_texts, handle)\n",
        "\n",
        "with open('/content/drive/MyDrive/persian-sentiment-analysis/train_labels.pickle', 'wb') as handle:\n",
        "    pickle.dump(train_labels, handle)\n",
        "\n",
        "with open('/content/drive/MyDrive/persian-sentiment-analysis/test_pad_sequences.pickle', 'wb') as handle:\n",
        "    pickle.dump(test_texts, handle)\n",
        "\n",
        "with open('/content/drive/MyDrive/persian-sentiment-analysis/test_labels.pickle', 'wb') as handle:\n",
        "    pickle.dump(test_labels, handle)\n",
        "\n",
        "with open('/content/drive/MyDrive/persian-sentiment-analysis/tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle)\n",
        "\n"
      ],
      "metadata": {
        "id": "nakjFBNXiiUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del train_texts\n",
        "del train_labels\n",
        "del test_texts\n",
        "del test_labels"
      ],
      "metadata": {
        "id": "wW1bzhyLiqU2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}